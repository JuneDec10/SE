缓存

高速缓存（， ）-{zh-cn:简称缓存，; zh-tw:，;}-原始意义是指存取速度比一般随机存取记忆体（RAM）快的一种RAM，通常它不像系统主记忆体那样使用DRAM技术，而使用昂贵但较快速的SRAM技术。

Cache一词来源于1967年的一篇电子工程期刊论文。其作者将法语词「cache」赋予“safekeeping storage”的涵义，用于电脑工程领域。PC-AT/XT和80286时代,没有Cache，CPU和内存都很慢，CPU直接访问内存。80386的芯片组增加了对可选的Cache的支持，高级主板带有64KB，甚至高端大气上档次的128KB Write-Through Cache。80486 CPU里面加入了8KB的L1 Unified Cache，当时也叫做内部Cache，不分代码和数据，都存在一起；芯片组中的Cache，变成了L2，也被叫做外部Cache，从128KB到256KB不等；增加了Write-back的Cache属性。Pentium CPU的L1 Cache分为Code和data，各自8KB；L2还被放在主板上。Pentium Pro的L2被放入到CPU的Package上。Pentium 4开始，L2 Cache被放入了CPU的Die中。从Intel Core CPU开始，L2 Cache为多核共享。

当CPU处理数据时，它会先到Cache中去寻找，如果数据因之前的操作已经读取而被暂存其中，就不需要再从随机存取记忆体（Main memory）中读取数据——由于CPU的运行速度一般比主内存的读取速度快，主存储器周期（访问主存储器所需要的时间）为数个时钟周期。因此若要存取主内存的话，就必须等待数个CPU周期从而造成浪费。

提供「缓存」的目的是为了让数据存取的速度适应CPU的处理速度，其基于的原理是内存中「程序执行与数据访问的局域性行为」，即一定程序执行时间和空间内，被访问的代码集中于一部分。为了充分发挥缓存的作用，不仅依靠“暂存刚刚访问过的数据”，还要使用硬件实现的指令预测与数据预取技术——尽可能把将要使用的数据预先从内存中取到缓存里。

CPU的缓存曾经是用在超级计算机上的一种高级技术，不过现今电脑上使用的的AMD或Intel微处理器都在芯片内部集成了大小不等的数据缓存和指令缓存，通称为L1缓存（L1 Cache即Level 1 On-die Cache，第一级片上高速缓冲存储器）；而比L1更大容量的L2缓存曾经被放在CPU外部（主板或者CPU接口卡上），但是现在已经成为CPU内部的标准组件；更昂贵的CPU会配备比L2缓存还要大的L3缓存（level 3 On-die Cache第三级高速缓冲存储器）。

如今缓存的概念已被扩充，不仅在CPU和主内存之间有Cache，而且在内存和硬盘之间也有Cache（磁盘缓存），乃至在硬盘与网络之间也有某种意义上的Cache──称为Internet临时文件夹或网络内容缓存等。凡是位于速度相差较大的两种硬件之间，用于协调两者数据传输速度差异的结构，均可称之为Cache。

由于主存容量远大于CPU缓存的容量，因此两者之间就必须按一定的规则对应起来。地址映象就是指按某种规则把主存块装入缓存中。地址变换是指当按某种映象方式把主存块装入缓存后，每次访问CPU缓存时，如何把主存的物理地址（Physical address）或虚拟地址（Virtual address）变换成CPU缓存的地址，从而访问其中的数据。

主存容量远大于CPU缓存，磁盘容量远大于主存，因此无论是哪一层次的缓存都面临一个同样的问题：当容量有限的缓存的空闲空间全部用完后，又有新的内容需要添加进缓存时，如何挑选并舍弃原有的部分内容，从而腾出空间放入这些新的内容。解决这个问题的算法有几种，如最久未使用算法（LFU）、先进先出算法（FIFO）、最近最少使用算法（LRU）、非最近使用算法（NMRU）等，这些算法在不同层次的缓存上执行时拥有不同的效率和代价，需根据具体场合选择最合适的一种。


