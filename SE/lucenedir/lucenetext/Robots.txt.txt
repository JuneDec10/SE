Robots.txt

robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。因为一些系统中的URL是大小写敏感的，所以robots.txt的文件名应统一为小写。robots.txt应放置于网站的根目录下。如果想单独定义搜索引擎的漫游器访问子目录时的行为，那么可以将自定的设置合并到根目录下的robots.txt，或者使用robots元数据（Metadata，又称元资料）。

robots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。注意robots.txt是用字符串比较来确定是否获取URL，所以目录末尾有与没有斜杠“/”表示的是不同的URL。robots.txt允许使用类似"Disallow: *.gif"这样的通配符。

其他的影响搜索引擎的行为的方法包括使用robots元数据：

这个协议也不是一个规范，而只是约定俗成的，有些搜索引擎会遵守这一规范，有些则不然。通常搜索引擎会识别这个元数据，不索引这个页面，以及这个页面的链出页面。

允许所有的机器人：

另一写法
仅允许特定的机器人：（name_spider用真实名字代替）
拦截所有的机器人：
禁止所有机器人访问特定目录：
仅禁止坏爬虫访问特定目录（BadBot用真实的名字代替）：
禁止所有机器人访问特定文件类型：
codice_1指令被几大搜索引擎支持（包括Google、Yahoo、Bing和Ask），指定了网站Sitemaps文件的位置。Sitemaps文件包含了网站页面所在的URL的一个列表。codice_1指令并不受codice_3指令的限制，所以它可以放在robots.txt文件中的任意位置。
唯一要注意的就是要使用网站地图指令，<nowiki><sitemap_location></nowiki>,并将URL的"location"值换成网站地图的地址，例如，下面就是一个网站地图指令的例子：

如何编写Sitemaps文件，请参考sitemaps.org上的说明。

几大抓取工具支持codice_4参数，设置为多少秒，以等待同服务器之间连续请求：

一些大的Crawlers支持一项Allow指令，可以抵消先前Disallow指令。比如Googlebot。

虽然robots.txt是最为广泛接受的方法，但也可以与robots META标签一起使用。robots META标签主要是针对一个独立的页面设定，与其他的META标签（如使用的语言、页面的描述、关键词等）一样，robots META标签也是放在页面的HEAD标签中，专门用来告诉搜索引擎robots如何抓取该页的内容。注

