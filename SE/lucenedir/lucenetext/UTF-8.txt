UTF-8

UTF-8（）是一种针对Unicode的可变长度字元编码，也是一种前缀码。它可以用来表示Unicode标准中的任何字元，且其编码中的第一个位元组仍与ASCII相容，这使得原来处理ASCII字元的软体无须或只须做少部份修改，即可继续使用。因此，它逐渐成为电子邮件、网页及其他储存或传送文字优先采用的编码。

UTF-8使用一至六个位元组为每个字符编码（尽管如此，2003年11月UTF-8被RFC 3629重新规范，只能使用原来Unicode定义的区域，U+0000到U+10FFFF，也就是说最多四个字节）：

对上述提及的第四种字元而言，UTF-8使用四至六个位元组来编码似乎太耗费资源了。但UTF-8对所有常用的字元都可以用三个位元组表示，而且它的另一种选择，UTF-16编码，对前述的第四种字符同样需要四个位元组来编码，所以要决定UTF-8或UTF-16哪种编码比较有效率，还要视所使用的字元的分布范围而定。不过，如果使用一些传统的压缩系统，比如DEFLATE，则这些不同编码系统间的的差异就变得微不足道了。若顾及传统压缩算法在压缩较短文字上的效果不大，可以考虑使用Unicode标准压缩格式（SCSU）。

网际网路工程工作小组（IETF）要求所有网际网路协议都必须支持UTF-8编码。互联网邮件联盟（IMC）建议所有电子邮件软件都支持UTF-8编码。

1992年初，为建立良好的位元组串编码系统以供多位元组字元集使用，开始了一个正式的研究。ISO/IEC 10646的初稿中有一个非必须的附录，名为UTF。当中包含了一个供32位元的字元使用的位元组串编码系统。这个编码方式的性能并不令人满意，但它提出了将0-127的范围保留给ASCII以相容旧系统的概念。

1992年7月，X/Open委员会XoJIG开始寻求一个较佳的编码系统。Unix系统实验室（USL）的Dave Prosser为此提出了一个编码系统的建议。它具备可更快速实作的特性，并引入一项新的改进。其中，7位元的ASCII符号只代表原来的意思，所有多位元组序列则会包含第8位元的符号，也就是所谓的最高有效位元。

1992年8月，这个建议由IBMX/Open的代表流传到一些感兴趣的团体。与此同时，贝尔实验室九号计划作业系统工作小组的肯·汤普逊对这编码系统作出重大的修改，让编码可以自我同步，使得不必从字串的开首读取，也能找出字符间的分界。1992年9月2日，肯·汤普逊和罗勃·派克一起在美国新泽西州一架餐车的餐桌垫上描绘出此设计的要点。接下来的日子，Pike及汤普逊将它实现，并将这编码系统完全应用在九号计划当中，及后他将有关成果回馈X/Open。

1993年1月25-29日的在圣地牙哥举行的USENIX会议首次正式介绍UTF-8。

自1996年起，微软的CAB（MS Cabinet）规格在UTF-8标准正式落实前就明确容许在任何地方使用UTF-8编码系统。但有关的编码器实际上从来没有实作这方面的规格。

目前有好几份关于UTF-8详细规格的文件，但这些文件在定义上有些许的不同：

它们取代了以下那些被淘汰的定义：

事实上，所有定义的基本原理都是相同的，它们之间最主要的不同是支持的字元范围及无效输入的处理方法。

Unicode字元的位元被分割为数个部分，并分配到UTF-8的位元组串中较低的位元的位置。在U+0080的以下字元都使用内含其字元的单位元组编码。这些编码正好对应7位元的ASCII字符。在其他情况，有可能需要多达4个字元组来表示一个字元。这些多位元组的最高有效位元会设定成1，以防止与7位元的ASCII字符混淆，并保持标准的位元组主导字串运作顺利。

例如，希伯来语字母aleph（א）的Unicode代码是U+05D0，按照以下方法改成UTF-8：

所以开始的128个字元（US-ASCII）只需一字节，接下来的1920个字符需要双字节编码，包括带附加符号的拉丁字母，希腊字母，西里尔字母，科普特语字母，亚美尼亚语字母，希伯来文字母和阿拉伯字母的字元。基本多文种平面中其余的字元使用三个字节，剩余字符使用四个字节。

根据这种方式可以处理更大数量的字元。原来的规范允许长达6字节的序列，可以覆盖到31位元（通用字符集原来的极限）。尽管如此，2003年11月UTF-8被RFC 3629重新规范，-{只}-能使用原来Unicode定义的区域，U+0000到U+10FFFF。根据这些规范，以下字节值将无法出现在合法UTF-8序列中：


因此，对UTF-8编码中的任意字节，根据第一位，可判断是否为ASCII字符；根据前二位，可判断该字节是否为一个字符编码的第一个字节；根据前四位（如果前两位均为1），可确定该字节为字符编码的第一个字节，并且可判断对应的字符由几个字节表示；根据前五位（如果前四位为1），可判断编码是否有错误或数据传输过程中是否有错误。

UTF-8的设计有以下的多字元组序列的特质：


UTF-8的这些特质，保证了一个字符的字节序列不会包含在另一个字符的字节序列中。这确保了以位元组为基础的部份字串比对（sub-string match）方法可以适用于在文字中搜寻字或词。有些比较旧的可变长度8位元编码（如Shift JIS）没有这个特质，故字串比对的算法变得相当复杂。虽然这增加了UTF-8编码的字串的信息冗余，但是利多于弊。另外，资料压缩并非Unicode的目的，所以不可混为一谈。即使在传送过程中有部份位元组因错误或干扰而完全遗失，还是有可能在下一个字符的起点重新同步，令受损范围受到限制。

另一方面，由于其位元组序列设计，如果一个疑似为字符串的序列被验证为UTF-8编码，那么我们可以有把握地说它是UTF-8字符串。一段两位元组随机序列碰巧为合法的UTF-8而非ASCII的机率为32分1。对于三位元组序列的机率为256分1，对更长的序列的机率就更低了。

UTF-8是UNICODE的一种变长度的编码表达方式一般UNICODE为双位元组（指UCS2），它由肯·汤普逊于1992年建立，现在已经标准化为RFC 3629。UTF-8就是以8位为单元对UCS进行编码，而UTF-8不使用大尾序和小尾序的形式，每个使用UTF-8储存的字符，除了第一个字节外，其余字节的头两个位元都是以"10"开始，使文字处理器能够较快地找出每个字符的开始位置。

但为了与以前的ASCII码相容（ASCII为一个位元组），因此UTF-8选择了使用可变长度字节来储存Unicode：

ASCII字母继续使用1字节储存，重音文字、希腊字母或西里尔字母等使用2字节来储存，而常用的汉字就要使用3字节。辅助平面字元则使用4字节。

在UTF-8+BOM格式文件的开首，很多时都放置一个U+FEFF字符（UTF-8以EF,BB,BF代表），以显示这个文字档案是以UTF-8编码。


总体来说，在Unicode字符串中不可能由码点数量决定显示它所需要的长度，或者显示字符串之后在文本缓冲区中光标应该放置的位置；组合字符、变宽字体、不可打印字符和从右至左的文字都是其归因。

所以尽管在UTF-8字符串中字元数量与码点数量的关系比UTF-32更为复杂，在实际中很少会遇到有不同的情形。

更详细的说，UTF-8编码具有以下几点优点：

一份写得很差（并且与当前标准的版本不兼容）的UTF-8解析器可能会接受一些不同的伪UTF-8表示并将它们转换到相同的Unicode输出上。这为设计用于处理八位表示的校验例程提供了一种遗漏信息的方式。

正则表达式可以进行很多英文高级的模糊检索。例如，[a-h]表示a到h间所有字母。

同样GBK编码的中文也可以这样利用正则表达式，比如在只知道一个字的读音而不知道怎么写的情况下，也可用正则表达式检索，因为GBK编码是按读音排序的。但是UTF-8不是按读音排序的，所以不利于用正则表达式检索（虽然正则表达式检索并未考虑中文中的多音字，但是由于中文的多音字数量不多，不少多音字还是同音不同调类型的多音字，所以大多数情况下正则表达式检索是可以接受的）。但是，Unicode是按部首排序的，因此在只知道一个字的部首而不知道如何发音的情况下，UTF-8可用正则表达式检索而GBK不行。

与其他Unicode编码相比，特别是UTF-16，在UTF-8中ASCII字元占用的空间-{只}-有一半，可是在一些字元的UTF-8编码占用的空间就要多出1/3，特别是中文、日文和韩文（CJK）这样的方块文字。

在资料库系统MySQL或MariaDB中有多种字符集，其中utf8_unicode_ci和utf8_general_ci是最常用的，但是utf8_general_ci对某些语言的支持有一些小问题，如果可以接受，那最好使用utf8_general_ci，因为它速度快。否则，请使用较为精确的utf8_unicode_ci，不过速度会慢一些。

虽然不是标准，但许多Windows程序（包括Windows记事本）在UTF-8编码的档案的开首加入一段位元组串codice_4。这是位元组顺序记号codice_5的UTF-8编码结果。对于没有预期要处理UTF-8的文字编辑器和浏览器会显示成ISO-8859-1字符串codice_6。

Posix系统明确不建议使用字节序掩码codice_4。因为很多文本文件期望以 “#!”开头指示要运行的程序。Linux系统选择使用Unicode规范形式Normalization Form C（NFC），即优先使用预组装字符（precomposed character）而非组合字符序列（combining character sequence）。

2002年9月发布的Red Hat Linux 8.0才开始正式把大多数区域设置的默认编码设为UTF-8。此前是各种语言的但字节编码为主。2004年9月SuSE Linux 9.1开始，缺省编码迁移为UTF-8。

字符串处理时，使用UTF-8或locale依赖的多字节编码情形，比使用C语言wchar_t的宽字符固定宽度编码，要慢1至2个数量级。

在通常用法下，Java程序语言在通过和读取和写入串的时候支持标准UTF-8。但是，Java也支持一种非标准的变体UTF-8，供对象的序列化，Java本地界面和在class文件中的嵌入常数时使用的。

标准和变种的UTF-8有两个不同点。第一，空字符（null character，U+0000）使用双字节的0xc0 0x80，而不是单字节的0x00。这保证了在已编码字串中没有嵌入空字节。因为C语言等语言程序中，单字节空字符是用来标志字串结尾的。当已编码字串放到这样的语言中处理，一个嵌入的空字符将把字串一刀两断。

第二个不同点是基本多文种平面之外字符的编码的方法。在标准UTF-8中，这些字符使用4字节形式编码，而在修正的UTF-8中，这些字符和UTF-16一样首先表示为代理对（surrogate pairs），然后再像CESU-8那样按照代理对分别编码。这样修正的原因更是微妙。Java中的字符为16位长，因此一些Unicode字符需要两个Java字符来表示。语言的这个性质盖过了Unicode的增补平面的要求。尽管如此，为了要保持良好的向后兼容、要改变也不容易了。这个修正的编码系统保证了一个已编码字串可以一次编为一个UTF-16码，而不是一次一个Unicode码点。不幸的是，这也意味着UTF-8中需要4字节的字符在变种UTF-8中变成需要6字节。

因为变种UTF-8并不是UTF-8，所以用户在交换信息和使用互联网的时候需要特别注意不要误把变种UTF-8当成UTF-8数据。

Mac OS X操作系统使用正式分解万国码（canonically decomposed Unicode），在文件系统中使用UTF-8编码进行文件命名，这做法通常被称为UTF-8-MAC。正式分解万国码中，预组合字符是被禁止使用的，必须以组合字符取代。

这种方法使分类变得非常简单，但是会搞混那些使用预组合字符为标准、组合字符用来显示特殊字符的软件。Mac系统的这种NFD数据是万国码规范化（Unicode normalization）的一种格式。而其他系统，包括Windows和Linux，使用万国码规范的NFC形式，也是W3C标准使用的形式。所以通常NFD数据必须转换成NFC才能被其他平台或者网络使用。

苹果开发者专区有关于此问题的讨论：Apple Q&A 1173。



